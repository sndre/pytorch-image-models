{"cells":[{"cell_type":"markdown","id":"sF8bvU6yG_F7","metadata":{"id":"sF8bvU6yG_F7"},"source":["# Connect to Drive"]},{"cell_type":"code","execution_count":1,"id":"Wkd8yo2CHDuJ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16688,"status":"ok","timestamp":1713745120804,"user":{"displayName":"Andriy Sergiyenko","userId":"15454250444236454786"},"user_tz":420},"id":"Wkd8yo2CHDuJ","outputId":"61715f58-7e89-4049-eabe-348c1643717a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/colab/pytorch-image-models\n"]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'colab/cs231n/assignments/assignment3/'\n","FOLDERNAME = 'colab/pytorch-image-models/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"code","execution_count":2,"id":"lID9DF_66UVi","metadata":{"id":"lID9DF_66UVi","executionInfo":{"status":"ok","timestamp":1713745121433,"user_tz":420,"elapsed":632,"user":{"displayName":"Andriy Sergiyenko","userId":"15454250444236454786"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"id":"1QGlLwWxBthc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1713745121649,"user":{"displayName":"Andriy Sergiyenko","userId":"15454250444236454786"},"user_tz":420},"id":"1QGlLwWxBthc","outputId":"6b213d48-5721-4b95-f0eb-0dd0b9f10abd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"]},{"cell_type":"markdown","id":"ov8Evcjh4v7T","metadata":{"id":"ov8Evcjh4v7T"},"source":["# Validate Google ViT Model"]},{"cell_type":"code","execution_count":null,"id":"KEM3Kfnl44Ij","metadata":{"id":"KEM3Kfnl44Ij"},"outputs":[],"source":["!pip install -e .\n","!pip install ml_collections"]},{"cell_type":"code","execution_count":5,"id":"WZqNH-HK41G4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZqNH-HK41G4","outputId":"e8d0af35-f1aa-4bb6-b67c-11aecbb10da2","executionInfo":{"status":"ok","timestamp":1713759053027,"user_tz":420,"elapsed":13351872,"user":{"displayName":"Andriy Sergiyenko","userId":"15454250444236454786"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["args: Namespace(data='datasets/imagenet1k/val', data_dir=None, dataset='', split='validation', num_samples=None, dataset_download=False, class_map='', input_key=None, input_img_mode=None, target_key=None, model='vit_base_patch16_224.orig_in21k_ft_in1k', pretrained=True, workers=2, batch_size=256, img_size=None, in_chans=None, input_size=None, use_train_size=False, crop_pct=None, crop_mode=None, crop_border_pixels=None, mean=None, std=None, interpolation='', num_classes=None, gp=None, log_freq=10, checkpoint='', num_gpu=1, test_pool=False, no_prefetcher=False, pin_mem=False, channels_last=False, device='cuda', amp=False, amp_dtype='float16', amp_impl='native', tf_preprocessing=False, use_ema=False, fuser='', fast_norm=False, reparam=False, model_kwargs={}, torchscript=False, torchcompile=None, aot_autograd=False, results_file='', results_format='csv', real_labels='', valid_labels='', retry=False, use_stratified_sampler=True, use_random_sampler=False, max_batches_for_pca=58)\n","Validating in float32. AMP not enabled.\n","args.model: vit_base_patch16_224.orig_in21k_ft_in1k args.pretrained: True args.num_classes: None in_chans: 3 args.gp: None args.torchscript: False args.model_kwargs: {} args.reparam: False args.test_pool: False args.channels_last: False args.torchscript: False args.torchcompile: None args.aot_autograd: False\n","Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.orig_in21k_ft_in1k)\n","model.safetensors: 100% 346M/346M [00:02<00:00, 156MB/s]\n","[timm/vit_base_patch16_224.orig_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n","Model vit_base_patch16_224.orig_in21k_ft_in1k created, param count: 86567656\n","Data processing configuration for current model + dataset:\n","\tinput_size: (3, 224, 224)\n","\tinterpolation: bicubic\n","\tmean: (0.5, 0.5, 0.5)\n","\tstd: (0.5, 0.5, 0.5)\n","\tcrop_pct: 0.9\n","\tcrop_mode: center\n","root_dir: datasets/imagenet1k/val args.dataset:  args.split: validation args.dataset_download: False args.tf_preprocessing: False args.class_map:  args.num_samples: None args.input_key: None input_img_mode: RGB args.target_key: None\n","args.valid_labels: \n","args.real_labels: \n","data_config['input_size']: (3, 224, 224) args.batch_size: 256 args.prefetcher: True data_config['interpolation']: bicubic data_config['mean']: (0.5, 0.5, 0.5) data_config['std']: (0.5, 0.5, 0.5) args.workers: 2 crop_pct: 0.9 data_config['crop_mode']: center args.crop_border_pixels: None args.pin_mem: False device: cuda args.tf_preprocessing: False\n","slicing model\n","Replacing layers\n","Replacing layers done\n","Fusing layernorm modules\n","Fusing layernorm modules done\n","using stratified sampler\n","Rotate and slice layers\n","Rotate and slice layers done\n","Model vit_base_patch16_224.orig_in21k_ft_in1k sliced, param count: 73768552\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Test: [   0/196]  Time: 129.283s (129.283s,    1.98/s)  Loss:  0.3257 (0.3257)  Acc@1:  91.016 ( 91.016)  Acc@5:  99.219 ( 99.219)\n","Test: [  10/196]  Time: 33.090s (69.870s,    3.66/s)  Loss:  0.6783 (0.5030)  Acc@1:  81.641 ( 85.156)  Acc@5:  95.703 ( 97.550)\n","Test: [  20/196]  Time: 2.528s (65.724s,    3.90/s)  Loss:  0.4382 (0.5236)  Acc@1:  91.797 ( 84.914)  Acc@5:  97.656 ( 97.414)\n","Test: [  30/196]  Time: 13.896s (66.357s,    3.86/s)  Loss:  0.6636 (0.4899)  Acc@1:  80.469 ( 85.925)  Acc@5:  96.875 ( 97.681)\n","Test: [  40/196]  Time: 18.736s (66.762s,    3.83/s)  Loss:  0.6829 (0.5656)  Acc@1:  82.812 ( 83.832)  Acc@5:  95.703 ( 97.313)\n","Test: [  50/196]  Time: 103.418s (68.261s,    3.75/s)  Loss:  0.5403 (0.6051)  Acc@1:  85.547 ( 82.690)  Acc@5:  96.484 ( 97.013)\n","Test: [  60/196]  Time: 132.328s (69.995s,    3.66/s)  Loss:  1.0805 (0.6388)  Acc@1:  71.094 ( 81.679)  Acc@5:  90.234 ( 96.811)\n","Test: [  70/196]  Time: 123.981s (70.414s,    3.64/s)  Loss:  0.6209 (0.6340)  Acc@1:  83.203 ( 81.971)  Acc@5:  98.828 ( 96.864)\n","Test: [  80/196]  Time: 132.122s (70.606s,    3.63/s)  Loss:  1.8817 (0.6768)  Acc@1:  55.859 ( 81.211)  Acc@5:  84.375 ( 96.383)\n","Test: [  90/196]  Time: 153.828s (71.007s,    3.61/s)  Loss:  1.5216 (0.7535)  Acc@1:  63.672 ( 79.653)  Acc@5:  86.328 ( 95.480)\n","Test: [ 100/196]  Time: 133.828s (70.447s,    3.63/s)  Loss:  1.2993 (0.8410)  Acc@1:  66.797 ( 78.013)  Acc@5:  87.500 ( 94.388)\n","Test: [ 110/196]  Time: 134.221s (69.884s,    3.66/s)  Loss:  0.8610 (0.8870)  Acc@1:  78.516 ( 77.048)  Acc@5:  92.969 ( 93.944)\n","Test: [ 120/196]  Time: 140.351s (69.523s,    3.68/s)  Loss:  1.6558 (0.9280)  Acc@1:  60.547 ( 76.437)  Acc@5:  86.328 ( 93.534)\n","Test: [ 130/196]  Time: 111.426s (68.935s,    3.71/s)  Loss:  0.7276 (0.9781)  Acc@1:  83.203 ( 75.388)  Acc@5:  95.312 ( 92.966)\n","Test: [ 140/196]  Time: 176.464s (69.158s,    3.70/s)  Loss:  1.0621 (1.0018)  Acc@1:  75.000 ( 75.028)  Acc@5:  92.969 ( 92.708)\n","Test: [ 150/196]  Time: 139.678s (69.165s,    3.70/s)  Loss:  1.3151 (1.0363)  Acc@1:  72.266 ( 74.397)  Acc@5:  88.281 ( 92.211)\n","Test: [ 160/196]  Time: 113.593s (68.977s,    3.71/s)  Loss:  1.1779 (1.0511)  Acc@1:  73.828 ( 74.076)  Acc@5:  89.844 ( 92.049)\n","Test: [ 170/196]  Time: 135.170s (68.863s,    3.72/s)  Loss:  0.9003 (1.0750)  Acc@1:  80.078 ( 73.593)  Acc@5:  93.359 ( 91.827)\n","Test: [ 180/196]  Time: 135.657s (68.615s,    3.73/s)  Loss:  1.9801 (1.0949)  Acc@1:  51.172 ( 73.202)  Acc@5:  82.812 ( 91.629)\n","Test: [ 190/196]  Time: 128.884s (68.334s,    3.75/s)  Loss:  0.9594 (1.1042)  Acc@1:  72.266 ( 73.090)  Acc@5:  96.875 ( 91.494)\n"," * Acc@1 73.256 (26.744) Acc@5 91.566 (8.434)\n","--result\n","{\n","    \"model\": \"vit_base_patch16_224.orig_in21k_ft_in1k\",\n","    \"top1\": 73.256,\n","    \"top1_err\": 26.744,\n","    \"top5\": 91.566,\n","    \"top5_err\": 8.434,\n","    \"param_count\": 73.77,\n","    \"img_size\": 224,\n","    \"crop_pct\": 0.9,\n","    \"interpolation\": \"bicubic\"\n","}\n"]}],"source":["!python validate.py datasets/imagenet1k/val --model vit_base_patch16_224.orig_in21k_ft_in1k --workers 2 --pretrained --use-stratified-sampler --max-batches-for-pca 58"]},{"cell_type":"code","source":["!python validate.py datasets/imagenet1k/val --model vit_base_patch16_224.orig_in21k_ft_in1k --workers 2 --pretrained --use-stratified-sampler --max-batches-for-pca 233"],"metadata":{"id":"i-jWTIdBjSyJ"},"id":"i-jWTIdBjSyJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python validate.py datasets/imagenet1k/val --model vit_base_patch16_224.orig_in21k_ft_in1k --workers 2 --pretrained --use-stratified-sampler --max-batches-for-pca 467"],"metadata":{"id":"6AOrs6hEjTjE"},"id":"6AOrs6hEjTjE","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sF8bvU6yG_F7"],"gpuType":"T4","provenance":[{"file_id":"https://huggingface.co/damian0815/CLIP-ViT-H-14-laion2B-s32B-b79K_CoreML/blob/main/clip-to-coreml.ipynb","timestamp":1703969858294}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}